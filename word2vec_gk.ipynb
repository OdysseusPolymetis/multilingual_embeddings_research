{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree as ET\n",
    "import lxml.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.corpus.greek.beta_to_unicode import Replacer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "author1=\"Aeschines\"\n",
    "author2=\"Aeschylus\"\n",
    "author3=\"Andocides\"\n",
    "author4=\"\"\n",
    "author5=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "tokenizer = TokenizeSentence('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Replacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if author1 != \"\":\n",
    "    texts1=\"\"\n",
    "    files= glob.iglob(author1+'/**/*gk.xml', recursive=True)\n",
    "    for filename in files :\n",
    "        p = ET.XMLParser(remove_blank_text=True, resolve_entities=False)\n",
    "        tree1 = ET.parse(filename,p)\n",
    "        root1 = tree1.find(\".//text\")\n",
    "        rawtext= lxml.html.tostring(root1, method=\"text\", encoding=\"utf8\")\n",
    "        texts1+=rawtext.decode()\n",
    "\n",
    "if author2 != \"\":\n",
    "    texts2=\"\"\n",
    "    files=glob.iglob(author2+'/**/*gk.xml', recursive=True)\n",
    "    for filename in files :\n",
    "        p = ET.XMLParser(remove_blank_text=True, resolve_entities=False)\n",
    "        tree = ET.parse(filename,p)\n",
    "        root = tree.find(\".//text\")\n",
    "        rawtext= lxml.html.tostring(root, method=\"text\", encoding=\"utf8\")\n",
    "        texts2+=rawtext.decode()\n",
    "\n",
    "if author3 != \"\":\n",
    "    texts3=\"\"\n",
    "    files=glob.iglob(author3+'/**/*gk.xml', recursive=True)\n",
    "    for filename in files :\n",
    "        p = ET.XMLParser(remove_blank_text=True, resolve_entities=False)\n",
    "        tree = ET.parse(filename,p)\n",
    "        root = tree.find(\".//text\")\n",
    "        rawtext= lxml.html.tostring(root, method=\"text\", encoding=\"utf8\")\n",
    "        texts3+=rawtext.decode()\n",
    "\n",
    "if author4 != \"\":\n",
    "    texts4=\"\"\n",
    "    files=glob.iglob(author4+'/**/*gk.xml', recursive=True)\n",
    "    for filename in files :\n",
    "        p = ET.XMLParser(remove_blank_text=True, resolve_entities=False)\n",
    "        tree = ET.parse(filename,p)\n",
    "        root = tree.find(\".//text\")\n",
    "        rawtext= lxml.html.tostring(root, method=\"text\", encoding=\"utf8\")\n",
    "        texts4+=rawtext.decode()\n",
    "\n",
    "if author5 != \"\":\n",
    "    texts5=\"\"\n",
    "    files=glob.iglob(author5+'/**/*gk.xml', recursive=True)\n",
    "    for filename in files :\n",
    "        p = ET.XMLParser(remove_blank_text=True, resolve_entities=False)\n",
    "        tree = ET.parse(filename,p)\n",
    "        root = tree.find(\".//text\")\n",
    "        rawtext= lxml.html.tostring(root, method=\"text\", encoding=\"utf8\")\n",
    "        texts5+=rawtext.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'texts1' in locals():\n",
    "    sentences1 = tokenizer.tokenize_sentences(texts1)\n",
    "if 'texts2' in locals():\n",
    "    sentences2 = tokenizer.tokenize_sentences(texts2)\n",
    "if 'texts3' in locals():\n",
    "    sentences3 = tokenizer.tokenize_sentences(texts3)\n",
    "if 'texts4' in locals():\n",
    "    sentences4 = tokenizer.tokenize_sentences(texts4)\n",
    "if 'texts5' in locals():\n",
    "    sentences5 = tokenizer.tokenize_sentences(texts5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1647\n",
      "1699\n",
      "1245\n"
     ]
    }
   ],
   "source": [
    "if 'sentences1' in locals():\n",
    "    print(len(sentences1))\n",
    "if 'sentences2' in locals():\n",
    "    print(len(sentences2))\n",
    "if 'sentences3' in locals():\n",
    "    print(len(sentences3))\n",
    "if 'sentences4' in locals():\n",
    "    print(len(sentences4))\n",
    "if 'sentences5' in locals():\n",
    "    print(len(sentences5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc1=set()\n",
    "voc2=set()\n",
    "voc3=set()\n",
    "voc4=set()\n",
    "voc5=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "word_tokenizer = WordTokenizer('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4591\n"
     ]
    }
   ],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "\n",
    "lemmatized_sentences=list()\n",
    "lemmatized_sentences1=list()\n",
    "lemmatized_sentences2=list()\n",
    "lemmatized_sentences3=list()\n",
    "lemmatized_sentences4=list()\n",
    "lemmatized_sentences5=list()\n",
    "    \n",
    "lemmatizer = LemmaReplacer('greek')\n",
    "\n",
    "    \n",
    "if 'sentences1' in locals():\n",
    "    for idx,sentence in enumerate(sentences1):\n",
    "        lemmatized_tokens = [r.beta_code(lemma) for lemma in lemmatizer.lemmatize(sentence) if len(lemma)>2 and lemma not in ['.', ',', ':', ';','','·',', ',')','(','*','<','>','[',']','—','\\'']]\n",
    "        voc1.update(lemmatized_tokens)\n",
    "        lemmatized_sentences1.append(lemmatized_tokens)\n",
    "        \n",
    "if 'sentences2' in locals():\n",
    "    for idx,sentence in enumerate(sentences2):\n",
    "        lemmatized_tokens = [r.beta_code(lemma) for lemma in lemmatizer.lemmatize(sentence) if len(lemma)>2 and lemma not in ['.', ',', ':', ';','','·',', ',')','(','*','<','>','[',']','—','\\'']]\n",
    "        voc2.update(lemmatized_tokens)\n",
    "        lemmatized_sentences2.append(lemmatized_tokens)\n",
    "\n",
    "if 'sentences3' in locals():\n",
    "    for idx,sentence in enumerate(sentences3):\n",
    "        lemmatized_tokens = [r.beta_code(lemma) for lemma in lemmatizer.lemmatize(sentence) if len(lemma)>2 and lemma not in ['.', ',', ':', ';','','·',', ',')','(','*','<','>','[',']','—','\\'']]\n",
    "        voc3.update(lemmatized_tokens)\n",
    "        lemmatized_sentences3.append(lemmatized_tokens)\n",
    "\n",
    "if 'sentences4' in locals():\n",
    "    for idx,sentence in enumerate(sentences4):\n",
    "        lemmatized_tokens = [r.beta_code(lemma) for lemma in lemmatizer.lemmatize(sentence) if len(lemma)>2 and lemma not in ['.', ',', ':', ';','','·',', ',')','(','*','<','>','[',']','—','\\'']]\n",
    "        voc4.update(lemmatized_tokens)\n",
    "        lemmatized_sentences4.append(lemmatized_tokens)\n",
    "\n",
    "if 'sentences5' in locals():\n",
    "    for idx,sentence in enumerate(sentences5):\n",
    "        lemmatized_tokens = [r.beta_code(lemma) for lemma in lemmatizer.lemmatize(sentence) if len(lemma)>2 and lemma not in ['.', ',', ':', ';','','·',', ',')','(','*','<','>','[',']','—','\\'']]\n",
    "        voc5.update(lemmatized_tokens)\n",
    "        lemmatized_sentences5.append(lemmatized_tokens)\n",
    "\n",
    "lemmatized_sentences=lemmatized_sentences1+lemmatized_sentences2+lemmatized_sentences3+lemmatized_sentences4+lemmatized_sentences5\n",
    "print(len(lemmatized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['μὲν', 'γὰρ', 'νόμοι', 'προεῖπον', 'τῷ', 'σχρῶς', 'βεβιωκότι', 'μὴ', 'δημηγορεῖν', 'πίταγμα', 'δὴ', 'γὼ', 'κρίνω', 'χαλεπὸν', 'πιτάξαντες', 'λλὰ', 'καὶ', 'πάνυ', 'ᾴδιον', 'μὲ', 'ξῆν', 'τῷ', 'σωφρόνει', 'μὴ', 'συκοφαντεῖν.']\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(lemmatized_sentences, min_count=10,max_vocab_size=10000, negative=5, iter=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=922, size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save(\"./model_gk.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltsv = gensim.models.KeyedVectors.load(\"./model_gk.bin\")\n",
    "\n",
    "with open(\"./tensorflow_gk.tsv\", 'w+') as tensors:\n",
    "    with open(\"./tensorflowmeta_gk.tsv\", 'w+') as metadata:\n",
    "         for word in modeltsv.index2word:\n",
    "                authors=set()\n",
    "                if word in voc1:\n",
    "                    authors.add(author1)\n",
    "                if word in voc2:\n",
    "                    authors.add(author2)\n",
    "                if word in voc3:\n",
    "                    authors.add(author3)\n",
    "                if word in voc4:\n",
    "                    authors.add(author4)\n",
    "                if word in voc5:\n",
    "                    authors.add(author5)\n",
    "                metadata.write(word+'_'+'_'.join(authors)+'\\n')\n",
    "                vector_row = '\\t'.join(map(str, modeltsv[word]))\n",
    "                tensors.write(vector_row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
