{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>Générer un espace sémantique multilingue à partir de corpus monolingues</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook qui permet de générer des modèles d'espace sémantique bilingue à partir de corpus monolingues, avec word2vec.\n",
    "<lb>Ce notebook a recours, entre autres, aux librairies <b>CLTK</b>, <b>gensim</b> et <b>word2vec</b>, et crée des sorties exploitables, entre autres, sur le projecteur d'embeddings de <a href : \"https://projector.tensorflow.org/\">tensorflow</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "import glob\n",
    "import string\n",
    "import unidecode\n",
    "from collections import defaultdict\n",
    "from cltk.corpus.utils.importer import CorpusImporter\n",
    "import pandas as pd\n",
    "import ipympl\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import numpy as np\n",
    "\n",
    "from lxml import etree as ET\n",
    "import lxml.html\n",
    "\n",
    "from cltk.corpus.greek.beta_to_unicode import Replacer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre suivant sert à déterminer la longueur des vecteurs à entraîner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_comp=500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Si vous n'avez pas encore importé les modèles de langue de CLTK\n",
    "Sinon, passez à la cellule suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_importer = CorpusImporter('greek')\n",
    "corpus_importer.import_corpus('greek_models_cltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.sentence import TokenizeSentence\n",
    "tokenizer = TokenizeSentence('greek')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings du grec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Replacer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_gk=\"homer_gk\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## si le texte grec est déjà lemmatisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si le texte est tiré de treebanks comme ceux de Perseus, disponibles sur le github de Perseus et déjà lemmatisés, vous pouvez utiliser cette partie. Sinon, allez à la suivante, qui utilise CLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if author_gk != \"\":\n",
    "    texts_gk=\"\"\n",
    "    lemmatized_sentences_gk=list()\n",
    "    files= glob.iglob(author_gk+'/**/*.xml', recursive=True)\n",
    "    for filename in files :\n",
    "        p = ET.XMLParser(remove_blank_text=True, resolve_entities=False)\n",
    "        tree_gk = ET.parse(filename,p)\n",
    "        sentences_gk = tree_gk.findall(\".//sentence\")\n",
    "        for sentence in sentences_gk:\n",
    "            words_per_sentence=list()\n",
    "            for word in sentence.xpath(\".//word/@lemma\"):\n",
    "                words_per_sentence.append(word)\n",
    "            lemmatized_sentences_gk.append(words_per_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized_sentences_gk[0])\n",
    "print(len(lemmatized_sentences_gk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## si le texte grec n'est pas lemmatisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if author1 != \"\":\n",
    "    texts1=\"\"\n",
    "    files= glob.iglob(author1+'/**/*gk.xml', recursive=True)\n",
    "    for filename in files :\n",
    "        p = ET.XMLParser(remove_blank_text=True, resolve_entities=False)\n",
    "        tree1 = ET.parse(filename,p)\n",
    "        root1 = tree1.find(\".//text\")\n",
    "        rawtext= lxml.html.tostring(root1, method=\"text\", encoding=\"utf8\")\n",
    "        texts1+=rawtext.decode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'texts1' in locals():\n",
    "    sentences1 = tokenizer.tokenize_sentences(texts1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sentences1' in locals():\n",
    "    print(len(sentences1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc1=set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.tokenize.word import WordTokenizer\n",
    "word_tokenizer = WordTokenizer('greek')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cltk.stem.lemma import LemmaReplacer\n",
    "\n",
    "lemmatized_sentences=list()\n",
    "    \n",
    "lemmatizer = LemmaReplacer('greek')\n",
    "\n",
    "    \n",
    "if 'sentences1' in locals():\n",
    "    for idx,sentence in enumerate(sentences1):\n",
    "        print(idx)\n",
    "        lemmatized_tokens = [r.beta_code(lemma) for lemma in lemmatizer.lemmatize(sentence) if len(lemma)>2 and lemma not in ['.', ',', ':', ';','','·',', ',')','(','*','<','>','[',']','—','\\'']]\n",
    "        print(lemmatizer.lemmatize(sentence))\n",
    "        voc1.update(lemmatized_tokens)\n",
    "        lemmatized_sentences.append(lemmatized_tokens)\n",
    "\n",
    "print(len(lemmatized_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatized_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement de Word2vec pour le grec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entraînement du premier corpus monolingue, en grec ancien, à partir des phrases lemmatisées. Par défaut, les termes pris en compte sont ceux qui apparaissent au moins dix fois, pour éviter les coquilles. Le vocabulaire est limité à 10000 mots pour le temps de calcul. On définit la taille des vecteurs avec le paramètre \"size\", ici par défaut 500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gk = Word2Vec(lemmatized_sentences_gk, min_count=10,max_vocab_size=10000, negative=5, iter=50, size=nb_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_gk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = list(model_gk.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gk.wv.save_word2vec_format(\"./model_gk.emb\", fvocab=None, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings du français"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les textes français sont lemmatisés et mis au format TEI souhaité avec ce taggeur personnel (écrit par Frédéric Glorieux, que j'ai adapté pour Python), disponible <a href=\"https://github.com/ANRChapitres/tagging\">ici</a>. Tous les textes sont mis dans un seul et même dossier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "author_fr = \"fr_translators\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_vocs={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if author_fr != \"\":\n",
    "    texts_fr=\"\"\n",
    "    lemmatized_sentences_fr=list()\n",
    "    files= glob.iglob(author_fr+'/**/*.xml', recursive=True)\n",
    "    \n",
    "    for filename in files :\n",
    "        author=filename[len(author_fr)+1:len(filename)-11]\n",
    "        p = ET.XMLParser(remove_blank_text=True, resolve_entities=False)\n",
    "        tree_fr = ET.parse(filename,p)\n",
    "        sentences_fr = tree_fr.findall(\".//sentence\")\n",
    "        for sentence in sentences_fr:\n",
    "            words_per_sentence = list()\n",
    "            for word in sentence.xpath(\".//word/@lemma\"):\n",
    "                if word not in string.punctuation and word is not \" \":\n",
    "                    words_per_sentence.append(word)\n",
    "                    if word in list_of_vocs.keys():\n",
    "                        list_of_vocs[word].add(author)\n",
    "                    else :\n",
    "                        list_of_vocs[word]=set()\n",
    "                        list_of_vocs[word].add(author)\n",
    "            lemmatized_sentences_fr.append(words_per_sentence)\n",
    "print(len(lemmatized_sentences_fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fr = Word2Vec(lemmatized_sentences_fr, min_count=10,max_vocab_size=10000, negative=5, iter=50, size=nb_comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fr.wv.save_word2vec_format(\"./model_fr.emb\", fvocab=None, binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection des embeddings sur un espace commun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la doc, voir ici : https://github.com/artetxem/vecmap\n",
    "<br/>Attention, si sur gros ordinateur équipé d'une GPU, ne pas oublier le flag \"--cuda\"\n",
    "<br/>Ce qui donnerait la commande subprocess.call(['python3', './vecmap/map_embeddings.py', '--unsupervised', '--cuda', source,target, 'src_mapped.emb', 'trg_mapped.emb'])\n",
    "<br/>Le cas échéant, ne pas oublier d'importer CuPy pour la gestion par GPU\n",
    "<br/><br/>\"Two equivalent words in different languages should have a similar distribution, and we can use this fact to induce the initial set of wordpairings\", \"Our goal is to learn the <a href=\"https://en.wikipedia.org/wiki/Transformation_matrix\">linear transformation matrices</a> WX and WZ so the mapped embeddings XWX and ZWZ are in the same cross-lingual space\", voir le <a href=\"https://aclweb.org/anthology/P18-1073\">papier d'Artetxe et altri</a>.\n",
    "<br/><br/>Choisir une des deux méthodes suivantes, si vous disposez ou non d'un petit dictionnaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semi_supervised_mapping(source,target) :\n",
    "    subprocess.call(['python3', './vecmap/map_embeddings.py', '--unsupervised','--cuda', source,target, 'src_mapped.emb', 'trg_mapped.emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unsupervised_mapping(source,target) :\n",
    "    subprocess.call(['python3', './vecmap/map_embeddings.py', '--semi_supervised','train.dict','--cuda', source,target, 'src_mapped.emb', 'trg_mapped.emb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unsupervised_mapping('model_fr.emb','model_gk.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fusionne les deux fichiers avec les vecteurs de chaque langue projetés dans le même espace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_vec_file='src_mapped.emb'\n",
    "fr_vec_file=\"trg_mapped.emb\"\n",
    "with open ('bilingual_vecs.txt','w+') as f_bil:\n",
    "    f_bil.write(str(len(model_gk.wv.vocab)+len(model_fr.wv.vocab))+\" \"+str(nb_comp)+\"\\n\")\n",
    "    glob=list()\n",
    "    with open(gk_vec_file, 'r') as f_gk:\n",
    "        glob.extend(f_gk.readlines()[1:])\n",
    "    with open(fr_vec_file,'r') as f_fr:\n",
    "        glob.extend(f_fr.readlines()[1:])\n",
    "    \n",
    "    for line in glob:\n",
    "        f_bil.write(line)\n",
    "    \n",
    "    f_bil.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des vecteurs de l'espace multilingue et démonstration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeltsv = gensim.models.KeyedVectors.load_word2vec_format('bilingual_vecs.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./tensorflow.tsv\", 'w+') as tensors:\n",
    "    with open(\"./tensorflowmeta.tsv\", 'w+') as metadata:\n",
    "         for word in modeltsv.index2word:\n",
    "                authors=\"\"\n",
    "                if word in list_of_vocs.keys():\n",
    "                    if len(list_of_vocs[word])<5:\n",
    "                        authors+=\"_\".join(list_of_vocs[word])\n",
    "                if (len(authors)>1):\n",
    "                    metadata.write(word+\"_\"+authors+'\\n')\n",
    "                else:\n",
    "                    metadata.write(word+'\\n')\n",
    "                vector_row = '\\t'.join(map(str, modeltsv[word]))\n",
    "                tensors.write(vector_row + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Odysseus', 0.6975877285003662),\n",
       " ('Ὀδυσσεύς', 0.5966129899024963),\n",
       " ('Pirée', 0.5505570769309998),\n",
       " ('Tèlémakhos', 0.5240138173103333),\n",
       " ('Piraeos', 0.5220750570297241),\n",
       " ('ὑφορβός', 0.510199785232544),\n",
       " ('Autolykos', 0.4850497543811798),\n",
       " ('Télémaque', 0.4747697114944458),\n",
       " ('Autolycos', 0.4697454869747162),\n",
       " ('Orsiloque', 0.4671053886413574)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"Ulysse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('chérir', 0.6728097796440125),\n",
       " ('τίω', 0.5826131105422974),\n",
       " ('honorer', 0.5664807558059692),\n",
       " ('protéger', 0.46612560749053955),\n",
       " ('haïr', 0.4527318477630615),\n",
       " ('τίνω', 0.44333603978157043),\n",
       " ('valoir', 0.4342774748802185),\n",
       " ('plaire', 0.42875194549560547),\n",
       " ('τιμάω', 0.4277810752391815),\n",
       " ('traiter', 0.4219663143157959)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"aimer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tuer', 0.5438829660415649),\n",
       " ('κατακτείνω', 0.5224876999855042),\n",
       " ('outrager', 0.5185527205467224),\n",
       " ('commis', 0.5084099173545837),\n",
       " ('tramer', 0.46658986806869507),\n",
       " ('égorger', 0.462814062833786),\n",
       " ('punir', 0.4557381272315979),\n",
       " ('mépriser', 0.44803065061569214),\n",
       " ('conspirer', 0.44266751408576965),\n",
       " ('déshonorer', 0.4417601525783539)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"assassiner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('οἶνος', 0.6359249949455261),\n",
       " ('breuvage', 0.6237227916717529),\n",
       " ('boisson', 0.5851397514343262),\n",
       " ('nectar', 0.5671843886375427),\n",
       " ('liqueur', 0.5542675852775574),\n",
       " ('lait', 0.5295891165733337),\n",
       " ('κρέας', 0.5283176898956299),\n",
       " ('μέθυ', 0.5217383503913879),\n",
       " ('ἐρυθρός', 0.5033228397369385),\n",
       " ('μελιηδής', 0.5014011859893799)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"vin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('λαός', 0.5341816544532776),\n",
       " ('compagnon', 0.49779969453811646),\n",
       " ('soldat', 0.4497343897819519),\n",
       " ('Argiens', 0.39662763476371765),\n",
       " ('στρατός', 0.3915303945541382),\n",
       " ('Grecs', 0.3909304440021515),\n",
       " ('nôtres', 0.3854723274707794),\n",
       " ('Danaens', 0.3830326795578003),\n",
       " ('gens', 0.3824494779109955),\n",
       " ('capitaine', 0.3746872842311859)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"guerrier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('σῖτος', 0.6523345708847046),\n",
       " ('pain', 0.5389895439147949),\n",
       " ('πίνω', 0.5167507529258728),\n",
       " ('aliment', 0.5109034180641174),\n",
       " ('boisson', 0.4662516117095947),\n",
       " ('provision', 0.4558088779449463),\n",
       " ('manger', 0.4495968520641327),\n",
       " ('κρέας', 0.44095897674560547),\n",
       " ('breuvage', 0.44063881039619446),\n",
       " ('φαγεῖν', 0.43504542112350464)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"nourriture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('·', 0.48861220479011536),\n",
       " ('dont', 0.45370054244995117),\n",
       " ('où', 0.3881946802139282),\n",
       " ('qui', 0.3793215751647949),\n",
       " ('δέ', 0.3743005394935608),\n",
       " ('pour', 0.36974287033081055),\n",
       " ('de', 0.3554162383079529),\n",
       " ('quand', 0.34876349568367004),\n",
       " ('lorsque', 0.3483029305934906),\n",
       " ('puis', 0.3376055359840393)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"et\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ὄφρα', 0.54485023021698),\n",
       " ('tandis', 0.4267917275428772),\n",
       " ('avant', 0.42552343010902405),\n",
       " ('alors', 0.41726064682006836),\n",
       " ('pour', 0.35444748401641846),\n",
       " ('parce', 0.34463194012641907),\n",
       " ('peur', 0.3236440122127533),\n",
       " ('sitôt', 0.3111719787120819),\n",
       " ('désireux', 0.31033483147621155),\n",
       " ('demain', 0.30295059084892273)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"afin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trépas', 0.5939536094665527),\n",
       " ('Parque', 0.5737640857696533),\n",
       " ('Kèr', 0.48828446865081787),\n",
       " ('νεῖκος', 0.46946752071380615),\n",
       " ('perte', 0.4430833160877228),\n",
       " ('meurtre', 0.4051223397254944),\n",
       " ('Parques', 0.3897401988506317),\n",
       " ('forfait', 0.3799719512462616),\n",
       " ('destin', 0.3790188729763031),\n",
       " ('Kère', 0.3777950704097748)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"mort\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vaisseau', 0.8183284401893616),\n",
       " ('navire', 0.7919580936431885),\n",
       " ('nef', 0.7286497354507446),\n",
       " ('croiseur', 0.7107780575752258),\n",
       " ('ναῦς', 0.6743136644363403),\n",
       " ('barque', 0.5556160807609558),\n",
       " ('bord', 0.538252592086792),\n",
       " ('galère', 0.5344588756561279),\n",
       " ('flotte', 0.5338149070739746),\n",
       " ('κλισία', 0.5305114984512329)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"bateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fille', 0.6464950442314148),\n",
       " ('παράκοιτις', 0.6018003225326538),\n",
       " ('νύμφη', 0.5571032166481018),\n",
       " ('ἐκγίγνομαι', 0.5498508810997009),\n",
       " ('κόρη', 0.5260941982269287),\n",
       " ('Ναυσικάα', 0.4998238682746887),\n",
       " ('εὐρύοπα', 0.49351561069488525),\n",
       " ('ὀπυίω', 0.4886321425437927),\n",
       " ('γαμβρός', 0.4871732294559479),\n",
       " ('Λητώ', 0.4824705719947815)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"θυγάτηρ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('σύ', 0.7144296765327454),\n",
       " ('moi', 0.6475143432617188),\n",
       " ('te', 0.5921909213066101),\n",
       " ('ἐμός', 0.5551868677139282),\n",
       " ('σός', 0.5544769167900085),\n",
       " ('ἐκεῖνος', 0.5518284440040588),\n",
       " ('vous', 0.5323278903961182),\n",
       " ('me', 0.5144422650337219),\n",
       " ('tu', 0.5044656991958618),\n",
       " ('δύστηνος', 0.502662718296051)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"ἐγώ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('immortel', 0.5389121174812317),\n",
       " ('ἄνθρωπος', 0.5159936547279358),\n",
       " ('Οὐρανίωνες', 0.502825140953064),\n",
       " ('μάκαρ', 0.48085817694664),\n",
       " ('θεός', 0.4569125473499298),\n",
       " ('γόνος', 0.4510040581226349),\n",
       " ('αἰγίοχος', 0.4306454062461853),\n",
       " ('αἰδοῖος', 0.41837960481643677),\n",
       " ('τέρας', 0.4159465432167053),\n",
       " ('πάτηρ', 0.40604519844055176)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"ἀθάνατος\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Jupiter', 0.6503325700759888),\n",
       " ('Zeus', 0.5428218245506287),\n",
       " ('ζεύς', 0.5213541984558105),\n",
       " ('Atlas', 0.4349021911621094),\n",
       " ('Κρόνος', 0.38605567812919617),\n",
       " ('τιμάω', 0.370660662651062),\n",
       " ('ὑπισχνέομαι', 0.3631303012371063),\n",
       " ('Ἥρα', 0.3576769530773163),\n",
       " ('Κρονίδης', 0.35384267568588257),\n",
       " ('θεός', 0.35296741127967834)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modeltsv.most_similar(\"Ζεύς\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projecteur local (mais mieux vaut utiliser celui de tensorflow) : partie en développement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gk_vec_file='src_mapped.emb'\n",
    "fr_vec_file=\"trg_mapped.emb\"\n",
    "with open ('bilingual_vecs_local.txt','w+') as f_bil:\n",
    "    glob=list()\n",
    "    with open(gk_vec_file, 'r') as f_gk:\n",
    "        glob.extend(f_gk.readlines()[1:])\n",
    "    with open(fr_vec_file,'r') as f_fr:\n",
    "        glob.extend(f_fr.readlines()[1:])\n",
    "    \n",
    "    for line in glob:\n",
    "        f_bil.write(line)\n",
    "    \n",
    "    f_bil.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = [30, 30]\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6813,)\n",
      "(6813, 500)\n",
      "[',' 'δέ' '·']\n",
      "[[-0.160082 0.011541 -0.146137 ... -0.005783399999999999 -0.0148075\n",
      "  -0.00371258]\n",
      " [0.162606 -0.020852000000000002 -0.0186189 ... -0.00668697 -0.00495149\n",
      "  0.0249705]\n",
      " [-0.036411599999999995 -0.10372200000000001 -0.0052026 ...\n",
      "  0.020937599999999997 -0.0043487 0.00652271]]\n"
     ]
    }
   ],
   "source": [
    "vectors = pd.read_csv('bilingual_vecs_local.txt', delimiter=' ', header=None).as_matrix()\n",
    "words = vectors[:,0]\n",
    "vectors = vectors[:,1:]\n",
    "print(words.shape)\n",
    "print(vectors.shape)\n",
    "print(words[:3])\n",
    "print(vectors[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_words_3d(vectors, words, plot = True):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(vectors[:,0], vectors[:,1], vectors[:,2], marker='.')\n",
    "    for i, word in enumerate(words):\n",
    "        ax.text(x=vectors[i,0], y=vectors[i,1], z=vectors[i,2], s=word)\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    return plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = TSNE(n_components=3, random_state=0, perplexity=15)\n",
    "if True:\n",
    "    X_tsne_3d = model.fit_transform(vectors)\n",
    "    np.savetxt('tsne_3d.txt', X_tsne_3d);\n",
    "else:\n",
    "    print(\"WARNING: loading stale vectors from tsne_3d.txt\")\n",
    "    X_tsne_3d = np.loadtxt('tsne_3d.txt');\n",
    "    \n",
    "npts = 20\n",
    "plot_words_3d(X_tsne_3d[0:npts,0:3], words[0:npts]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "226b0745050647259423023dbfc5034b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to  previous…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ev = None\n",
    "\n",
    "# show interactive 3D plot like in Tensorboard\n",
    "def plot_words_3d_interactive(vectors, words, plot = True, n_neighbors = 15):\n",
    "    fig = plt.figure(figsize=(15,45))\n",
    "    ax = fig.add_subplot(211, projection='3d')\n",
    "    ax_zoom = fig.add_subplot(212, projection='3d')\n",
    "    ax.scatter(vectors[:,0], vectors[:,1], vectors[:,2], marker='.', c='b', picker=3)\n",
    "    if plot:\n",
    "        plt.show()\n",
    "    \n",
    "    NN_model = NearestNeighbors(n_neighbors=n_neighbors, algorithm='ball_tree').fit(vectors)\n",
    "    last_indices = None\n",
    "    \n",
    "    def onclick_3d_interactive(event):\n",
    "        _x, _y, _z = event.artist._offsets3d\n",
    "        ind = event.ind[0]\n",
    "        x = _x[ind]\n",
    "        y = _y[ind]\n",
    "        z = _z[ind]\n",
    "        distances, indices = NN_model.kneighbors([[x, y, z]], n_neighbors=n_neighbors)\n",
    "        distances = distances[0]\n",
    "        indices = indices[0]\n",
    "        if indices.size == 0:\n",
    "            return\n",
    "\n",
    "        avg_neighbor = np.mean(distances[1:6])\n",
    "        keep = distances>2*avg_neighbor\n",
    "        distance = distances[keep]\n",
    "        indices = indices[keep]\n",
    "        \n",
    "        ax_zoom.clear()\n",
    "        ax_zoom.scatter(vectors[indices,0], vectors[indices,1], vectors[indices,2], marker='.')\n",
    "        for i in indices:\n",
    "            jitter = 0\n",
    "            ax_zoom.text(x=vectors[i,0], y=vectors[i,1], z=vectors[i,2]+jitter, s=words[i])\n",
    "\n",
    "    cid = fig.canvas.mpl_connect('pick_event', onclick_3d_interactive)\n",
    "    return plt\n",
    "\n",
    "plt.close(\"all\")\n",
    "npts = 1000\n",
    "plot_words_3d_interactive(X_tsne_3d[0:npts,0:3], words[0:npts]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
